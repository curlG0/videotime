# VideoTime
This repository implemented a novel approach to tackle cold-start problem for video recommender system. Currently, recommender systems heavily rely on collaborative filtering. Both user-based and content-based collaborative filtering is dependent on click/use information and user information. This led to cold-start problems for recommender systems. When a new user joins or a new piece of content is added to the platform, the recommender system can't draw any inferences on them given the limited information. Some common solution to the cold-start problem include: to extract user information from interviews, extract side social relationships, representatives-based method. In this project, I'm tackling this problem by extracting rich semantics directly from videos, index and search over semantics (+ metadata) via Elasticsearch.

Specifically, the following steps are taken: 

1. Obtain frames from videos (1fps by default).
2. Extract semantics of the frames in text format by caption generation.
3. Index metadata and captions with Elasticsearch.
4. Provide ranking of video content so that unviewed videos can be recommended to users.

__Why Elasticsearch?__

Existing semantic-based content recommendation/ranking methods commonly involve constructing high-dimentional shared vector space to represent users/contents and then solving matrices in the space to rank or cluster users/contents. Euclidean distance is assumed to represent the similarity between contents/users, which is usually calculated using cosine similarity. Such approach is computationaly expensive and not suitable for streaming use cases. By consuming semantics in text format in Elasticsearch, the complexity of recommending m pieces of contents at is reduced from O(n) ~ O(n^2) (depending on the size of m), to approximately O(1).  

__How are captions generated?__

The captions are generated by implementing <a href="https://arxiv.org/abs/1612.00563" title="Self-critical Sequence Training for Image Captioning"> Self-critical Sequence Training for Image Captioning</a> where a form of reinforcement learning is used where the baseline rewards is estimated from test-time inference output. 


## Features
- __Usability:__ faster inference via Elastic query
- __Scalable:__ can use existing search/indexing engine from your company/project, light-weight index
- __Flexible:__ no GPU required for inference (GPU option available) 

## Installation

```
./setup.py install
```

Run:

```
videotime
```

## Dev setup

Setup conda environment:
```
conda create -n VideoTime python=3.7
conda activate VideoTime
```

Install dependencies:

```
pip install -r requirements.txt
```

## Run tests

```
./setup.py test
```

## Usage

### Run locally

```
docker-compose up -d dejavu
python -m videotime.main
```

### Process new videos

```
curl localhost:5000/process -H 'content-type: application/json' --data '{"url": "https://www.youtube.com/watch?v=3dcli9i_pvA"}'
```
